{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NC6tvwgk3UGd"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Manipulation and Processing\n",
        "\n",
        "All machine learning (ML) models concern themselves with extracting information from data. In order to build a successful ML model, we need to learn the practical skills for storing, manipulating and preprocessing data. \n",
        "Additionally, since all ML models work with large datasets (think of them as really large tables whose rows correspond to examples and whose colums corresponds to  attributes), we will need to know how to manipulate the data.\n",
        "**Linear Algebra** will become an important tool for us because it facilitates working with such large datasets. We will focus on matrix operations and implementation in Python using Pytorch. (Unit 3)\n",
        "\n",
        "Additionally, ML models are all about optimization. We start with a model with some randomly assigned parameters and we would like to tune those parameters (we called them knobs last class) so that our model predictions fit our data *the best*. Determining which way to move each parameter at each step of the algorithm requires some **basic calculus** which we will introduce. (Unit 4)\n",
        "\n",
        "Finally, ML models always try to make predictions. Intrinsically, the models try to predict the likelyhood of event occuring given some attributes observed in the data. This is achieved by using **probability** concepts. (Unit 5)\n",
        "\n",
        "## Data Manipulation\n",
        "To get started with any ML model we need a way to store and manipulate data. There are two important things we need to do with data: \n",
        "* acquire data\n",
        "* process the data acquired once they are inside the computer\n",
        "\n",
        "In order to store data, we introduce the $n$-dimensional array, which we also call a *tensor*. Think of a tensor as a programming object that can store multiple values at once. In this class, we will be using Pytorch's Tensor since it supports GPU and automatic differentiation which we will need later. \n",
        "\n",
        "\n",
        "### Getting Started \n",
        "To start, we import `torch`.\n",
        "Pytorch is an open source ML framework based on Python and Torch library. \n",
        "Torch is an open source ML library useed to create deep neural networks.\n",
        "The PyTorch framework supports over 200 different mathematical operations. PyTorch's popularity continues to rise, as it simplifies the creation of artificial neural network models. PyTorch is mainly used by data scientists for research and artificial intelligence (AI) applications."
      ],
      "metadata": {
        "id": "ucBUSfT13myU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch "
      ],
      "metadata": {
        "id": "kGrslOrv822Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A tensor represents an array of numerical values. If a tensor consists of one axis, we call that tensor a *vector*. If the tensor consists of two axis, we call that tensor a *matrix*. If the tensor consists of $k>2$ axes, we refer to the object as a *$k^{th}$ order tensor*.\n",
        "\n",
        "Pytorch provides a variety of functions for creating new tensors prepopulated with values."
      ],
      "metadata": {
        "id": "aigERD1x88Mg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.arange(12,dtype=torch.float32) # Command creates a vector of evenly spaced values, starting at 0 and ending at 12, with 12 not included\n",
        "print(x)"
      ],
      "metadata": {
        "id": "EZQJTn4W87oY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can access the tensor's shape (the length along each axis by calling on its `shape` property"
      ],
      "metadata": {
        "id": "a987n4YKRV2O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x.shape"
      ],
      "metadata": {
        "id": "SKPWzMlrRMLP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we just want to know the total number of elements in a tensor,  meaning the product of all of the shape elements, we can inspect its size. Because we are dealing with a vector here, the single element\n",
        "of its shape is identical to its size."
      ],
      "metadata": {
        "id": "pvYlbakaRq-t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x.numel()"
      ],
      "metadata": {
        "id": "HDAtKdJxRidC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To change the shape of a tensor without altering either the number of elements or their values, we\n",
        "can invoke the `reshape` function. For example, we can transform our tensor, x, from a row vector\n",
        "with shape (12,) to a matrix with shape (3, 4). This new tensor contains the exact same values, but\n",
        "views them as a matrix organized as 3 rows and 4 columns. To reiterate, although the shape has\n",
        "changed, the elements have not. Note that the size is unaltered by reshaping."
      ],
      "metadata": {
        "id": "NtYfcXjBTNXX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = x.reshape(3,4)\n",
        "X"
      ],
      "metadata": {
        "id": "LV1onrzKRyGc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sometimes specifying every dimension is unnecessary. If our target shape is a matrix of shape (height, width) and we know the width, the height is given implicitly. For example,\n",
        "`x.reshape(3,4)`, `x.reshape(-1,4)` and `x.reshape(3,-1)` all lead to a tensor of $3$ rows and $4$ columns. We invoke this capability by placing -1 for\n",
        "the dimension that we would like tensors to automatically infer."
      ],
      "metadata": {
        "id": "GTLl1nN_Ur0G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x.reshape(-1,4)"
      ],
      "metadata": {
        "id": "HpIh2eC-Teh2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x.reshape(3,-1)"
      ],
      "metadata": {
        "id": "5QBMc050VXzz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Typically, we will want our matrices initialized either with zeros, ones, some other constants, or numbers randomly sampled from a specific distribution. We can create a tensor representing a\n",
        "tensor with all elements set to 0 and a shape of (2, 3, 4) as follows:"
      ],
      "metadata": {
        "id": "Xozuu0h8V03r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.zeros((2,3,4))"
      ],
      "metadata": {
        "id": "_fLJUJYPVZkS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Similarly, we can create tensors with each element set to 1 as follows:"
      ],
      "metadata": {
        "id": "Lh1n6Q7zWKv8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.ones((2, 3, 4))"
      ],
      "metadata": {
        "id": "YyI-xWTpWE6-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Often, we want to randomly sample the values for each element in a tensor from some probability\n",
        "distribution. For example, when we construct arrays to serve as parameters in a neural network,\n",
        "we will typically initialize their values randomly. The following snippet creates a tensor with shape\n",
        "(3, 4). Each of its elements is randomly sampled from a standard Gaussian (normal) distribution \n",
        "with a mean of 0 and a standard deviation of 1."
      ],
      "metadata": {
        "id": "o6K3TR6xWSUY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.randn(3,4)"
      ],
      "metadata": {
        "id": "ApD8ipczWUw5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also specify the exact values for each element in the desired tensor by supplying a Python\n",
        "list (or list of lists) containing the numerical values. Here, the outermost list corresponds to axis\n",
        "0, and the inner list to axis 1."
      ],
      "metadata": {
        "id": "lAyDsAxYfWZ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.tensor([[2,1,4,3],[1,2,3,4],[4,3,2,1]])"
      ],
      "metadata": {
        "id": "4BKT_MshWW7l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Operations\n",
        "We want to perform mathematical operations on those arrays. Some\n",
        "of the simplest and most useful operations are the elementwise operations. These apply a standard\n",
        "scalar operation to each element of an array.\n",
        "\n",
        "The common standard arithmetic operators `(+, -, *, /, and **)` have all been lifted to elementwise\n",
        "operations."
      ],
      "metadata": {
        "id": "46rY9tUsf3Jx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.tensor([1.0, 2, 4, 8])\n",
        "y = torch.tensor([2, 2, 2, 2])\n",
        "print(x+y)\n",
        "print(x-y)\n",
        "print(x*y)\n",
        "print(x/y)\n",
        "print(x**y)"
      ],
      "metadata": {
        "id": "pRWkNFmGfguU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Many more operations can be applied elementwise, including unary operators like exponentiation or taking the sine or cosine of each element of the tensor. "
      ],
      "metadata": {
        "id": "ZLltmVUFlvvZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(torch.exp(x))\n",
        "print(torch.sin(x))"
      ],
      "metadata": {
        "id": "iyN-mjgQlioO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also concatenate multiple tensors together, stacking them end-to-end to form a larger tensor.\n",
        "We just need to provide a list of tensors and tell the system along which axis to concatenate.\n",
        "The example below shows what happens when we concatenate two matrices along rows (axis 0,\n",
        "the first element of the shape) vs. columns (axis 1, the second element of the shape). We can see\n",
        "that the first output tensorʼs axis-0 length (6) is the sum of the two input tensorsʼ axis-0 lengths\n",
        "(3+3); while the second output tensorʼs axis-1 length (8) is the sum of the two input tensorsʼ axis-1\n",
        "lengths (4 + 4)."
      ],
      "metadata": {
        "id": "epWe0W6tm1wE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = torch.arange(12,dtype=torch.float32).reshape((3,4))\n",
        "Y = torch.tensor([[2.0, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])\n",
        "Z = torch.cat((X, Y), dim=0) # concatenate X and Y along rows\n",
        "W = torch.cat((X,Y),dim = 1) # concatenate X and & along columns\n",
        "print(Z)\n",
        "print(W)"
      ],
      "metadata": {
        "id": "4MYrmoGIl3k5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sometimes, we want to construct a binary tensor via logical statements. Take X == Y as an example.\n",
        "For each position, if X and Y are equal at that position, the corresponding entry in the new tensor\n",
        "takes a value of 1, meaning that the logical statement X == Y is true at that position; otherwise that position is 0"
      ],
      "metadata": {
        "id": "nwh98ln5n-bV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X==Y"
      ],
      "metadata": {
        "id": "dQMUHSBgnhf5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Summing all the elements in the tensor yields a tensor with only one element."
      ],
      "metadata": {
        "id": "hAMdbY5uoLma"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X.sum()"
      ],
      "metadata": {
        "id": "Qq7r4CKdoDEN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Broadacsting Mechanism\n",
        "\n",
        "So far we showed how to perform elementwise operations for tensors of the same shape. If the shapes of the tensor are different however, we run into trouble. \n",
        "If we want to perform an arithmetic operation on two tensors of different sizes we can do so by invoking the *broadcasting mechanism*. \n",
        "We first expand one or both tensors by copying the elements appropriately so that after the transformation, the two tensors have the same shape. After, we carry out the elementwise operations on the resulting tensors. "
      ],
      "metadata": {
        "id": "iAgc75cPpDpT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.arange(3).reshape((3, 1))\n",
        "b = torch.arange(2).reshape((1, 2))\n",
        "a, b"
      ],
      "metadata": {
        "id": "YwJpH4pRoOje"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a+b"
      ],
      "metadata": {
        "id": "j6QitfJrp4V4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Indexing and Slicing\n",
        "Elements in a tensor can be accessed by index. The first element has index 0 and ranges are specified to include the first but not the last\n",
        "element. We can access elements according to their relative position to the end of the list by using negative indices.\n",
        "\n",
        "Thus, `[-1]` selects the last element and `[1:3]` selects the second and the third elements as follows"
      ],
      "metadata": {
        "id": "_-fVewwUqB9N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X,X[-1]"
      ],
      "metadata": {
        "id": "etJiX38qp5aH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X[1:3]"
      ],
      "metadata": {
        "id": "Kkjy9gDEqtYK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also write elements of a matrix by specifying indices"
      ],
      "metadata": {
        "id": "yHk1FWLSrF2c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X[1, 2] = 9\n",
        "X"
      ],
      "metadata": {
        "id": "1dB7I3v_qxJO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we want to assign multiple elements the same value, we simply index all of them and then assign\n",
        "them the value. For instance, `[0:2, :]` accesses the first and second rows, where `:` takes all the\n",
        "elements along axis 1 (column)."
      ],
      "metadata": {
        "id": "fOx887bwrRh0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X[0:2, :] = 12\n",
        "X"
      ],
      "metadata": {
        "id": "URHcYpPIrJrz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Saving Memory\n",
        "Running operations can cause new memory to be allocated to host results. For example, if we\n",
        "write `Y = X + Y`, we will dereference the tensor that Y used to point to and instead point Y at\n",
        "the newly allocated memory.\n",
        "\n",
        "Pythonʼs `id()` gives us the exact address of the referenced object in memory. After running `Y = Y + X`, we will find that `id(Y)` points to a different location. That is because Python first evaluates `Y+ X`, allocating new memory for the result and then makes Y point to this new location in memory."
      ],
      "metadata": {
        "id": "p8fK1dCZtGdC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "before = id(Y)\n",
        "Y = Y+X\n",
        "print(id(Y),before)"
      ],
      "metadata": {
        "id": "phmGDkb2rYrP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This might be undesirable for two reasons. \n",
        "* We do not want to run around allocating memory\n",
        "unnecessarily all the time. In machine learning, we might have hundreds of megabytes of\n",
        "parameters and update all of them multiple times per second. Typically, we will want to perform\n",
        "these updates in place.\n",
        "\n",
        "* We might point at the same parameters from multiple variables.\n",
        "If we do not update in place, other references will still point to the old memory location, making\n",
        "it possible for parts of our code to inadvertently reference stale parameters.\n",
        "\n",
        "Performing in-place operations is easy. We can assign the result of an operation to a previously allocated array with slice notation, e.g., `Y[:] = <expression>`. To illustrate this concept, we first create a new matrix Z with the same shape as another Y, using zeros_like to allocate a block of 0 entries."
      ],
      "metadata": {
        "id": "x0MgnVdcuE6f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Z = torch.zeros_like(Y)\n",
        "print('id(Z):', id(Z))\n",
        "Z[:] = X+Y\n",
        "print('id(Z):',id(Z))"
      ],
      "metadata": {
        "id": "Xw1KCT6AtZDW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Conversion to Other Python Objects\n",
        "\n",
        "Converting to a NumPy tensor (ndarray), or vice versa, is easy. The torch Tensor and numpy array\n",
        "will share their underlying memory locations, and changing one through an in-place operation\n",
        "will also change the other."
      ],
      "metadata": {
        "id": "t57cXxuhvW1P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "A = X.numpy()\n",
        "B = torch.from_numpy(A)\n",
        "type(A), type(B)\n"
      ],
      "metadata": {
        "id": "vljgOHXCvFTo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To convert a size-1 tensor to a Python scalar, we can invoke the item function or Pythonʼs built-in\n",
        "functions."
      ],
      "metadata": {
        "id": "9a-21OMrvqlH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.tensor([3.5])\n",
        "a, a.item(), float(a), int(a)"
      ],
      "metadata": {
        "id": "339vqjJFvsZi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preprocessing\n",
        "\n",
        "In deep learning we begin with preprocessing raw data, rather than working with nicely prepared data in the tensor format. The most commonly used data analytics tool is the `pandas`package in Python. The nice thing is that `pandas` works great with tensors. \n",
        "\n",
        "### Reading the dataset\n",
        "We begin by creating an artificial dataset that is stored in a \".csv\" (comma separated values) file. Data stored in other formats may be processed in similar ways. \n",
        "\n",
        "The code below writes the dataset row by row into a .csv file. "
      ],
      "metadata": {
        "id": "Xei-H18Ow1X-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.makedirs(os.path.join('..','data'),exist_ok=True)\n",
        "data_file = os.path.join('..','data', 'house_tiny.csv')\n",
        "with open(data_file,'w') as f:\n",
        "    f.write('NumRooms,Alley,Price\\n') #column names\n",
        "    f.write('NA,Pave,127500\\n') # Each row represents a data example\n",
        "    f.write('2,NA,106000\\n')\n",
        "    f.write('4,NA,178100\\n')\n",
        "    f.write('NA,NA,140000\\n')"
      ],
      "metadata": {
        "id": "D7HA1xbdxrh7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We load the raw dataset from the created csv file by first importing the pandas package and invoke the\n",
        "read_csv function. \n",
        "\n",
        "Note that this dataset has four rows and three columns, where each row describes the\n",
        "number of rooms (“NumRooms”), the alley type (“Alley”), and the price (“Price”) of a house."
      ],
      "metadata": {
        "id": "TIujeQC0yl_B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "data = pd.read_csv(data_file)\n",
        "print(data)"
      ],
      "metadata": {
        "id": "44eV7rlfydRp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Handling Missing Data\n",
        "Note that “NaN” entries are missing values. To handle missing data, typical methods include *imputation* and *deletion*\n",
        "* **Imputation** replaces missing values with substituted ones such as the mean of that specific column or we input zeros instead. \n",
        "* **Deletion** ignores missing values. \n",
        "\n",
        "Here we will consider imputation. First however, we split data into inputs and outputs, where the former\n",
        "takes the first two columns while the latter only keeps the last column. For numerical values in\n",
        "inputs that are missing, we replace the “NaN” entries with the mean value of the same column.\n",
        "Note that  `iloc` stands for index location and we will use that feature to split the dataset."
      ],
      "metadata": {
        "id": "qkTiCVuCzSXJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = data.iloc[:,0:2]\n",
        "outputs = data.iloc[:,2]\n",
        "print(inputs)\n",
        "print(outputs)"
      ],
      "metadata": {
        "id": "SSjFn5rky1MT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For categorical or discrete values in inputs, we consider “NaN” as a category. Since the “Alley”\n",
        "column only takes two types of categorical values “Pave” and “NaN”, `pandas` can automatically\n",
        "convert this column to two columns “Alley_Pave” and “Alley_nan”. A row whose alley type is “Pave”\n",
        "will set values of “Alley_Pave” and “Alley_nan” to 1 and 0. A row with a missing alley type will set\n",
        "their values to 0 and 1."
      ],
      "metadata": {
        "id": "qOCXZG1z1T3g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = pd.get_dummies(inputs,dummy_na = True)\n",
        "print(inputs)\n"
      ],
      "metadata": {
        "id": "cBvXH_F60lu7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We would like to inpute the 'NaN' values in \" NumRooms\" with the average of the column"
      ],
      "metadata": {
        "id": "LbHxmU-H3pmH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mean_NumRooms = inputs['NumRooms'].mean()\n",
        "inputs['NumRooms'].fillna(value=mean_NumRooms,inplace=True)"
      ],
      "metadata": {
        "id": "X-ifmZwI1jBZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(inputs)"
      ],
      "metadata": {
        "id": "xkkzC0Da2vYy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that all the entries in inputs and outputs are numerical, they can be converted to the tensor\n",
        "format."
      ],
      "metadata": {
        "id": "_RVML2MB4WsI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X, y = torch.tensor(inputs.values), torch.tensor(outputs.values)\n",
        "X, y"
      ],
      "metadata": {
        "id": "I-CiroUV4Nxk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KROsco1A4d6s"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}